import cv2
import mediapipe as mp
import numpy as np
import tensorflow as tf
from PIL import ImageFont, ImageDraw, Image

# ==========================================
# ì„¤ì •
# ==========================================
SEQ_LEN = 15
CONF_THRESHOLD = 0.60   # softmax ê¸°ì¤€
DIST_THRESHOLD_PATH = "model/distance_threshold.npy"
CENTROID_PATH = "model/centroids.npy"
MODEL_PATH = "model/best_cnn_gru_model.keras"
EMBED_MODEL_PATH = "model/embedding_model.keras"

CLASS_NAMES = [
    "ã„±","ã„´","ã„·","ã„¹","ã…","ã…‚","ã……","ã…‡","ã…ˆ","ã…Š","ã…‹","ã…Œ","ã…","ã…",
    "ã„²","ã„¸","ã…ƒ","ã…†","ã…‰",
    "ã…","ã…","ã…‘","ã…’","ã…“","ã…”","ã…•","ã…–",
    "ã…—","ã…˜","ã…™","ã…š","ã…›",
    "ã…œ","ã…","ã…","ã…Ÿ","ã… ",
    "ã…¡","ã…¢","ã…£"
]

# ==========================================
# íŒŒì¼ ë¡œë“œ
# ==========================================
print("ğŸ”„ ëª¨ë¸ ë¡œë“œ ì¤‘...")
model = tf.keras.models.load_model(MODEL_PATH)
embed_model = tf.keras.models.load_model(EMBED_MODEL_PATH)
centroids = np.load(CENTROID_PATH, allow_pickle=True).item()
distance_threshold = float(np.load(DIST_THRESHOLD_PATH))

print("âœ… ëª¨ë¸ / ì„ë² ë”© / ì„¼íŠ¸ë¡œì´ë“œ / ì„ê³„ê°’ ë¡œë“œ ì™„ë£Œ!")

# ==========================================
# ë¯¸ë””ì–´íŒŒì´í”„ ì„¤ì •
# ==========================================
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(max_num_hands=1,
                       min_detection_confidence=0.5,
                       min_tracking_confidence=0.5)
mp_drawing = mp.solutions.drawing_utils

# ==========================================
# ì „ì²˜ë¦¬ í•¨ìˆ˜
# ==========================================
def normalize_landmarks(landmarks):
    data = np.array(landmarks)
    wrist = data[0]
    data = data - wrist
    scale = np.linalg.norm(data[9])
    if scale < 1e-6: scale = 1e-6
    return (data / scale).flatten()

# ==========================================
# í•œê¸€ ì¶œë ¥ í•¨ìˆ˜
# ==========================================
def putText_korean(img, text, position, font_size=30, color=(0,255,0)):
    img_pil = Image.fromarray(img)
    draw = ImageDraw.Draw(img_pil)
    try:
        font = ImageFont.truetype("malgun.ttf", font_size)
    except:
        try:
            font = ImageFont.truetype("AppleGothic.ttf", font_size)
        except:
            font = ImageFont.load_default()
    draw.text(position, text, font=font, fill=color)
    return np.array(img_pil)

# ==========================================
# OOD Reject í•¨ìˆ˜
# ==========================================
def predict_with_ood(x):
    """
    x: (1, SEQ_LEN, 63)
    return: dict
    """
    softmax = model.predict(x, verbose=0)[0]
    max_prob = np.max(softmax)
    pred_idx = np.argmax(softmax)

    # 1) Softmax ê¸°ë°˜ Reject
    if max_prob < CONF_THRESHOLD:
        return {"result": "Reject (Low confidence)", "prob": float(max_prob)}

    # 2) Embedding ê¸°ë°˜ Reject
    emb = embed_model.predict(x, verbose=0)[0]
    dist = np.linalg.norm(emb - centroids[pred_idx])

    if dist > distance_threshold:
        return {
            "result": "Reject (Far from centroid)",
            "prob": float(max_prob),
            "distance": float(dist)
        }

    return {
        "result": CLASS_NAMES[pred_idx],
        "prob": float(max_prob),
        "distance": float(dist)
    }

# ==========================================
# ë©”ì¸ ë£¨í”„
# ==========================================
cap = cv2.VideoCapture(0)
sequence = []
locked_result = None

print("ğŸ¥ ì›¹ìº  ì‹œì‘ (ì¢…ë£Œ: q)")

while cap.isOpened():
    ret, frame = cap.read()
    if not ret: break

    frame = cv2.flip(frame, 1)
    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    results = hands.process(rgb)

    display_text = "ì†ì„ ì˜¬ë ¤ì£¼ì„¸ìš”."
    text_color = (200,200,200)

    if results.multi_hand_landmarks:
        hand = results.multi_hand_landmarks[0]
        mp_drawing.draw_landmarks(frame, hand, mp_hands.HAND_CONNECTIONS)

        # ì¢Œí‘œ ì¶”ì¶œ
        lm = [[lm.x, lm.y, lm.z] for lm in hand.landmark]
        normalized = normalize_landmarks(lm)

        sequence.append(normalized)
        if len(sequence) > SEQ_LEN:
            sequence.pop(0)

        # â˜… ì´ë¯¸ Lock ë˜ì–´ ìˆìœ¼ë©´ ê³„ì† ë™ì¼í•œ ê²°ê³¼ ìœ ì§€
        if locked_result is not None:
            display_text = f"{locked_result}"
            text_color = (0,255,0)

        elif len(sequence) == SEQ_LEN:
            x = np.expand_dims(np.array(sequence), 0)

            result = predict_with_ood(x)

            # Rejectì´ë©´ í™”ë©´ í‘œì‹œë§Œ í•˜ê³  lock ì•ˆí•¨
            if result["result"].startswith("Reject"):
                display_text = result["result"]
                text_color = (0,255,255)

            else:
                # ì •ìƒ ì§€í™” â†’ Lock
                locked_result = result["result"]
                display_text = locked_result
                text_color = (0,255,0)

    else:
        sequence = []
        locked_result = None

    # ìƒë‹¨ ë°°ê²½
    cv2.rectangle(frame, (0,0), (640,60), (0,0,0), -1)
    frame = putText_korean(frame, display_text, (20,15), 30, text_color)

    cv2.imshow("Real-Time Sign Recognition", frame)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
